import os
import json
import glob
from neo4j import GraphDatabase
from tqdm import tqdm

# Neo4j ì ‘ì† ì •ë³´
URI = "bolt://localhost:7687"
AUTH = ("neo4j", "password")

class GraphBuilder:
    def __init__(self, uri, auth):
        self.driver = GraphDatabase.driver(uri, auth=auth)
        self.verify_connection()

    def verify_connection(self):
        try:
            self.driver.verify_connectivity()
            print("âœ… Neo4j ì ‘ì† ì„±ê³µ!")
        except Exception as e:
            print(f"âŒ Neo4j ì ‘ì† ì‹¤íŒ¨: {e}")
            raise e

    def close(self):
        self.driver.close()

    def create_indexes(self):
        queries = [
            "CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE",
            "CREATE INDEX IF NOT EXISTS FOR (d:Document) ON (d.category)",
            "CREATE INDEX IF NOT EXISTS FOR (d:Document) ON (d.type)"
        ]
        with self.driver.session() as session:
            for q in queries:
                session.run(q)
        print("âœ… ì¸ë±ìŠ¤ ì„¤ì • ì™„ë£Œ")

    def load_processed_data(self):
        # ê²½ë¡œ: ai/data/processed
        current_dir = os.path.dirname(os.path.abspath(__file__))
        data_path = os.path.join(current_dir, "..", "data", "processed")
        
        print(f"ğŸ” ë°ì´í„° ê²½ë¡œ: {os.path.abspath(data_path)}")
        files = glob.glob(os.path.join(data_path, "**", "*.json"), recursive=True)
        all_chunks = []
        
        print(f"ğŸ“‚ íŒŒì¼ ìŠ¤ìº” ì¤‘... ({len(files)}ê°œ ë°œê²¬)")
        
        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        all_chunks.extend(data)
                    else:
                        all_chunks.append(data)
            except Exception as e:
                print(f"âš ï¸ ì½ê¸° ì‹¤íŒ¨: {file_path}")
        
        # [ë””ë²„ê¹…ìš©] ì²« ë²ˆì§¸ ë°ì´í„° êµ¬ì¡° í™•ì¸
        if len(all_chunks) > 0:
            print(f"ğŸ‘€ ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ (í‚¤ í™•ì¸): {list(all_chunks[0].keys())}")
            
        print(f"ğŸ“Š ì´ {len(all_chunks)}ê°œì˜ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        return all_chunks

    def create_nodes(self, chunks):
        # [ìˆ˜ì •ë¨] coalesce í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ metadataê°€ ìˆë“  ì—†ë“  ë°ì´í„°ë¥¼ ì°¾ì•„ë‚´ë„ë¡ ë³€ê²½
        query = """
        UNWIND $batch AS row
        MERGE (d:Document {id: row.chunk_id})
        SET d.content = row.content,
            d.source = coalesce(row.metadata.source, row.source, 'Unknown'),
            d.category = coalesce(row.metadata.category, row.category, 'General'),
            d.type = coalesce(row.metadata.type, row.type, 'document'),
            d.page = coalesce(row.metadata.page, row.page, 1)
        """
        batch_size = 500
        
        cleaned = []
        for i, c in enumerate(chunks):
            if 'chunk_id' not in c:
                c['chunk_id'] = f"unknown_{i}"
            cleaned.append(c)

        print("ğŸš€ Neo4jì— ë°ì´í„° ì €ì¥ ì‹œì‘...")
        with self.driver.session() as session:
            for i in tqdm(range(0, len(cleaned), batch_size), desc="Graph Node ìƒì„±"):
                batch = cleaned[i:i+batch_size]
                session.run(query, batch=batch)
        print("ğŸ‰ ì €ì¥ ì™„ë£Œ!")

def main():
    builder = GraphBuilder(URI, AUTH)
    try:
        builder.create_indexes()
        chunks = builder.load_processed_data()
        if chunks:
            builder.create_nodes(chunks)
        else:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    finally:
        builder.close()

if __name__ == "__main__":
    main()